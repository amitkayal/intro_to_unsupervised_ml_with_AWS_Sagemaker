{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Ali Parandeh - Beginners Machine Learning - London\n",
    "\n",
    "# Introduction to Unsupervised Machine Learning with AWS Sagemaker\n",
    "In this interesting 3hr workshop, you will take the massive dataset of UFO sightings (80,000 reports over the past century) from [National UFO Reporting Center (NUFORC)](http://www.nuforc.org/) and use Amazon's machine learning services ([AWS Sagemaker](https://aws.amazon.com/sagemaker/)) to identify the top 10 locations that are most likely to have UFO sightings. To do so, you will need to use an unsupervised machine learning algorithm.\n",
    "\n",
    "You will then take your trained model, deserialise it, convert its output to a csv format and visualise it on a map using AWS [Quicksight](https://aws.amazon.com/quicksight/) to see where these locations are. Then you can try correlating these locations with landmarks.\n",
    "\n",
    "The general machine learning workflow with AWS Sagemaker is shown below. For this assignment we will not evaluate or deploy the model but only use its output to visualise the results on a world map.\n",
    "\n",
    "<img src=\"https://docs.aws.amazon.com/sagemaker/latest/dg/images/ml-concepts-10.png\">\n",
    "\n",
    "### What is Unsupervised Machine Learning? \n",
    "\n",
    "With unsupervised learning, data features are fed into the learning algorithm, which determines how to label them (usually with numbers 0,1,2..) and based on what. This “based on what” part dictates which unsupervised learning algorithm to follow.\n",
    "\n",
    "Most unsupervised learning-based applications utilize the sub-field called **Clustering**. \n",
    "\n",
    "One of the most famous topics under the realm of Unsupervised Learning in Machine Learning is k-Means Clustering. Even though this clustering algorithm is fairly simple, it can look challenging to newcomers into the field. \n",
    "\n",
    "### What is the difference between supervised and unsupervised machine learning?\n",
    "\n",
    "The main difference between Supervised and Unsupervised learning algorithms is the absence of data labels in the latter.\n",
    "\n",
    "### What does clustering mean?\n",
    "\n",
    "**Clustering** is the process of grouping data samples together into clusters based on a certain feature that they share — exactly the purpose of unsupervised learning in the first place.\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*tWaaZX75oumVwBMcKN-eHA.png\">\n",
    "\n",
    "Source: [Clustering using K-means algorithm](https://towardsdatascience.com/clustering-using-k-means-algorithm-81da00f156f6)\n",
    "\n",
    "### How does the K-Means Algorithm work?\n",
    "\n",
    "Being a clustering algorithm, **k-Means** takes data points as input and groups them into `k` clusters. This process of grouping is the training phase of the learning algorithm. The result would be a model that takes a data sample as input and returns the cluster that the new data point belongs to, according the training that the model went through.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/700/1*6EOTS1IE2ULWC9SKgf7mYw.png\">\n",
    "\n",
    "Source - [How Does k-Means Clustering in Machine Learning Work?](https://towardsdatascience.com/how-does-k-means-clustering-in-machine-learning-work-fdaaaf5acfa0)\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/700/1*4LOxZL6bFl3rXlr2uCiKlQ.gif\">\n",
    "\n",
    "Source: [How Does k-Means Clustering in Machine Learning Work?](https://towardsdatascience.com/how-does-k-means-clustering-in-machine-learning-work-fdaaaf5acfa0)\n",
    "\n",
    "Check out the the two articles below to learn more about how the K-Means Algorithm work:\n",
    "\n",
    "- [Clustering using K-means algorithm](https://towardsdatascience.com/clustering-using-k-means-algorithm-81da00f156f6)\n",
    "- [How Does k-Means Clustering in Machine Learning Work?](https://towardsdatascience.com/how-does-k-means-clustering-in-machine-learning-work-fdaaaf5acfa0)\n",
    "\n",
    "\n",
    "### Where can you use k-means?\n",
    "\n",
    "The **k-means algorithm** can be a good fit for finding patterns or groups in large datasets that have not been explicitly labeled. Here are some example use cases in different domains:\n",
    "\n",
    "**E-commerce**\n",
    "\n",
    "- Classifying customers by purchase history or clickstream activity.\n",
    "\n",
    "**Healthcare**\n",
    "\n",
    "- Detecting patterns for diseases or success treatment scenarios.\n",
    "- Grouping similar images for image detection.\n",
    "\n",
    "**Finance**\n",
    "\n",
    "- Detecting fraud by detecting anomalies in the dataset. For example, detecting credit card frauds by abnormal purchase patterns.\n",
    "\n",
    "**Technology**\n",
    "\n",
    "- Building a network intrusion detection system that aims to identify attacks or malicious activity.\n",
    "\n",
    "**Meteorology**\n",
    "\n",
    "- Detecting anomalies in sensor data collection such as storm forecasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Importing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment, we need to import all the required packages: \n",
    "\n",
    "- **Amazon SageMaker Python SDK**: Amazon SageMaker Python SDK is an open source library for training and deploying machine-learned models on Amazon SageMaker. See [Documentation](https://sagemaker.readthedocs.io/en/stable/index.html)\n",
    "- **MXNet**: A flexible and efficient library for deep learning. - See [Documentation](https://mxnet.apache.org/versions/master/api/python/index.html)\n",
    "- **Python Built-in Library** [datetime](https://docs.python.org/2/library/datetime.html)\n",
    "- **Numpy** and **Pandas**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "from datetime import datetime\n",
    "\n",
    "import sagemaker.amazon.common as smac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise:** Construct a url to the the dataset location in your S3 bucket using the following expression and save it to `data_location`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://YOUR_OWN_BUCKET_NAME/ufo_dataset/ufo_complete.csv'"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket = \"YOUR_OWN_BUCKET_NAME\"\n",
    "prefix = \"ufo_dataset\"\n",
    "data_key = \"ufo_complete.csv\"\n",
    "\n",
    "# Construct a url string and save it to data_location variable\n",
    "data_location = \"s3://{}/{}/{}\".format(bucket, prefix, data_key)\n",
    "\n",
    "# print data_location\n",
    "print(data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Internally do not process the file in chunks when loading the csv onto \n",
    "# a dataframe to ensure avoid mixed type inferences when importing the large UFO dataset. \n",
    "df = pd.read_csv(data_location, low_memory= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "      <th>shape</th>\n",
       "      <th>duration (seconds)</th>\n",
       "      <th>duration (hours/min)</th>\n",
       "      <th>comments</th>\n",
       "      <th>date posted</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10/10/1949 20:30</td>\n",
       "      <td>san marcos</td>\n",
       "      <td>tx</td>\n",
       "      <td>us</td>\n",
       "      <td>cylinder</td>\n",
       "      <td>2700</td>\n",
       "      <td>45 minutes</td>\n",
       "      <td>This event took place in early fall around 194...</td>\n",
       "      <td>4/27/2004</td>\n",
       "      <td>29.8830556</td>\n",
       "      <td>-97.941111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10/10/1949 21:00</td>\n",
       "      <td>lackland afb</td>\n",
       "      <td>tx</td>\n",
       "      <td>NaN</td>\n",
       "      <td>light</td>\n",
       "      <td>7200</td>\n",
       "      <td>1-2 hrs</td>\n",
       "      <td>1949 Lackland AFB&amp;#44 TX.  Lights racing acros...</td>\n",
       "      <td>12/16/2005</td>\n",
       "      <td>29.38421</td>\n",
       "      <td>-98.581082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10/10/1955 17:00</td>\n",
       "      <td>chester (uk/england)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>gb</td>\n",
       "      <td>circle</td>\n",
       "      <td>20</td>\n",
       "      <td>20 seconds</td>\n",
       "      <td>Green/Orange circular disc over Chester&amp;#44 En...</td>\n",
       "      <td>1/21/2008</td>\n",
       "      <td>53.2</td>\n",
       "      <td>-2.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10/10/1956 21:00</td>\n",
       "      <td>edna</td>\n",
       "      <td>tx</td>\n",
       "      <td>us</td>\n",
       "      <td>circle</td>\n",
       "      <td>20</td>\n",
       "      <td>1/2 hour</td>\n",
       "      <td>My older brother and twin sister were leaving ...</td>\n",
       "      <td>1/17/2004</td>\n",
       "      <td>28.9783333</td>\n",
       "      <td>-96.645833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10/10/1960 20:00</td>\n",
       "      <td>kaneohe</td>\n",
       "      <td>hi</td>\n",
       "      <td>us</td>\n",
       "      <td>light</td>\n",
       "      <td>900</td>\n",
       "      <td>15 minutes</td>\n",
       "      <td>AS a Marine 1st Lt. flying an FJ4B fighter/att...</td>\n",
       "      <td>1/22/2004</td>\n",
       "      <td>21.4180556</td>\n",
       "      <td>-157.803611</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           datetime                  city  ...    latitude   longitude\n",
       "0  10/10/1949 20:30            san marcos  ...  29.8830556  -97.941111\n",
       "1  10/10/1949 21:00          lackland afb  ...    29.38421  -98.581082\n",
       "2  10/10/1955 17:00  chester (uk/england)  ...        53.2   -2.916667\n",
       "3  10/10/1956 21:00                  edna  ...  28.9783333  -96.645833\n",
       "4  10/10/1960 20:00               kaneohe  ...  21.4180556 -157.803611\n",
       "\n",
       "[5 rows x 11 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the head of the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88875, 11)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the shape of the dataframe\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Clearning, transforming and preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the 'latitude' and 'longitude' columns and save it as a new dataframe `df_geo`.\n",
    "df_geo = df[[\"latitude\", \"longitude\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29.8830556</td>\n",
       "      <td>-97.941111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29.38421</td>\n",
       "      <td>-98.581082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>53.2</td>\n",
       "      <td>-2.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28.9783333</td>\n",
       "      <td>-96.645833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21.4180556</td>\n",
       "      <td>-157.803611</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     latitude   longitude\n",
       "0  29.8830556  -97.941111\n",
       "1    29.38421  -98.581082\n",
       "2        53.2   -2.916667\n",
       "3  28.9783333  -96.645833\n",
       "4  21.4180556 -157.803611"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the head of df_geo\n",
    "df_geo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 88875 entries, 0 to 88874\n",
      "Data columns (total 2 columns):\n",
      "latitude     88875 non-null object\n",
      "longitude    88875 non-null float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# Fully inspect the df_geo dataframe\n",
    "df_geo.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon successfull inspection of the above dataframe, you should notice two errors with this dataframe:\n",
    "\n",
    "- There are no `null` or missing values in both columns. However, we still need to check for other incorrect entries that are not **coordinates**. Example: `0`, `string`s, etc.\n",
    "- The `latitude` column has a `dtype` of `object`. This means the column may have missing or string values where the rest of the values are numbers. If the entries in the column are non-homogenous, pandas will store the column as a `string` or `object` data type. To clean the data in this column we can use pandas' `.to_numeric()` method to convert the data in this column to `float` for processing. The machine learning algorithm expects data passed in to it to be numerical digits `float`s or `int`s not `string`s. - See [Documentation] on how to use this method.\n",
    "\n",
    "> **Exercise:** Convert the `latitude` column datatype to `float`. You can pass in the `errors = \"coerce\"` option to `.to_numeric()` method to enforce the conversion. When conversion is not possible - i.e. values are strings - these strings will be replaced with NaNs. Therefore, chain a `.dropna()` method to drop rows where `NaNs` exist. Then check whether the column formats have been converted to numerical data types `float` and if any missing values are still present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the column values to numeric and whenever it is not possible replace the value with NaNs and then drop rows\n",
    "# where NaNs exist\n",
    "\n",
    "df_geo.to_numeric(inplace = True, errors = \"Coerce\").dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of null values in the dataframe - Expecting this to be zero\n",
    "print(df_geo.isnull().any().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Re-checking the dataframe to ensure both columns have numerical datatype such as `float` or `int`.\n",
    "df_geo.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are there any missing values? False\n"
     ]
    }
   ],
   "source": [
    "# Check if we have any missing values (NaNs) in our dataframe\n",
    "missing_values = df_geo.isnull().values.any()\n",
    "print(\"Are there any missing values? {}\".format(missing_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there are any missing values in the dataframe, show them\n",
    "if (missing_values):\n",
    "    df_geo[df_geo.isnull().any(axis = 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 29.883055, -97.94111 ],\n",
       "       [ 29.38421 , -98.581085],\n",
       "       [ 53.2     ,  -2.916667],\n",
       "       ...,\n",
       "       [ 35.65278 , -97.477776],\n",
       "       [ 34.376945, -82.69583 ],\n",
       "       [ 26.121944, -80.14361 ]], dtype=float32)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# store the cleaned up dataframe column values as a 2D numpy array (matrix) with datatype of float32\n",
    "data_train = df_geo.values.astype(\"float32\")\n",
    "\n",
    "# Print the 2D numpy array\n",
    "print(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create and train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of clusters and output location URL to save the trained model\n",
    "num_clusters = 10\n",
    "output_location = \"s3://\" + bucket + \"/model-artifacts\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To pass a model training command to Amazon for training, we need to grab the details of the current execution role **ARN ID** whose credentials we are using to call the Sagemaker API. \n",
    "\n",
    "> **Exercise:** Grab the ARN ID of your current Execution role using the `sagemaker` SDK - See [Documentation](https://sagemaker.readthedocs.io/en/stable/session.html?highlight=get%20execution#sagemaker.session.get_execution_role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the execution role ARN ID to pass to the sagemaker API later on\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# Check that you have this step correctly performed\n",
    "print(role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now can use Amazon's built-in K-means ML algorithm to find `k` clusters of data in our unlabeled UFO dataset.\n",
    "\n",
    "Amazon SageMaker uses a modified version of the web-scale k-means clustering algorithm. Compared with the original version of the algorithm, the version used by Amazon SageMaker is more accurate. Like the original algorithm, it scales to massive datasets and delivers improvements in training time. To do this, the version used by Amazon SageMaker streams mini-batches (small, random subsets) of the training data. The k-means algorithm expects tabular data, where rows represent the observations that you want to cluster, and the columns represent attributes of the observations. See [Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/k-means.html)\n",
    "\n",
    "To ask AWS sagemaker for training a model using this algorithm we need to define a **K-means Estimator**. KMeans Estimators can be configured by setting **hyperparameters** which are arguments passed into the Estimator Constructor Function. \n",
    "\n",
    "This estimator requires the following hyperparameters to be passed in `sagemaker.KMeans()`:\n",
    "\n",
    "- `role` (str) – An AWS IAM role (either name or full ARN). The Amazon SageMaker training jobs and APIs that create Amazon SageMaker endpoints use this role to access training data and model artifacts. After the endpoint is created, the inference code might use the IAM role, if accessing AWS resource.\n",
    "- `train_instance_count` (int) – Number of Amazon EC2 instances to use for training.\n",
    "- `train_instance_type` (str) – Type of EC2 instance to use for training, for example, ‘ml.c4.xlarge’. This is the **compute resources** that you want Amazon SageMaker to use for model training. Compute resources are ML compute instances that are managed by Amazon SageMaker.\n",
    "- `k` (int) – The number of clusters to produce.\n",
    "- `output_path` (str) - The URL of the S3 bucket where you want to store the output of the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training API request to AWS Sagemaker\n",
    "kmeans = sagemaker.KMeans(role = role,\n",
    "               train_instance_count = 1,\n",
    "               train_instance_type = \"ml.c4.xlarge\",\n",
    "               output_path = output_location,\n",
    "               k = num_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following diagram shows how you train and deploy a model with Amazon SageMakern - See [Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-training.html)\n",
    "\n",
    "<img src=\"https://docs.aws.amazon.com/sagemaker/latest/dg/images/sagemaker-architecture-training-2.png\">\n",
    "\n",
    "To train a model in Amazon SageMaker, you create a **training job** using the `kmeans.fit()` method. - See [Documentation](https://sagemaker.readthedocs.io/en/stable/kmeans.html?highlight=kmeans.fit#sagemaker.KMeans.fit)\n",
    "\n",
    "The training job requires the following information passed in to `.fit()` method:\n",
    "\n",
    "- `record_set(data_train)` (str) - The training records to train the KMeans Estimator on. Here `data_train` must be passed in to the `kmeans.record_set()` method to convert our 2D numpy array data to a `RecordSet` object that is required by the algorithm. - See [Documentation](https://sagemaker.readthedocs.io/en/stable/sagemaker.amazon.amazon_estimator.html?highlight=record_set()#sagemaker.amazon.amazon_estimator.AmazonAlgorithmEstimatorBase.record_set)\n",
    "- `job_name` (str) - Training job name. If not specified, the estimator generates a default job name, based on the training image name and current timestamp.\n",
    "\n",
    "Amazon SageMaker then launches the ML compute instances and uses the training code and the training dataset to train the model. It saves the resulting model artifacts and other output in the S3 bucket you specified for that purpose.\n",
    "\n",
    "Here we are going to construct a job name using the following expression and Python's built-in `datetime` module. This ensures our `job_name` is unique even if the code cell below is run multiple times. Each training job requires a **unique** `job_name`. Otherwise, AWS will throw an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the job name: kmeans-geo-job-20190729005440\n"
     ]
    }
   ],
   "source": [
    "job_name = \"kmeans-geo-job-{}\".format(datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
    "print(\"Here is the job name: {}\".format(job_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise**: Create a training job using `kmeans.fit()`. Use the AWS documentation links above to figure out how to pass in the arguments to `kmeans.fit()` for the training job to commence. \n",
    "\n",
    "If you do this step right, you should see outputs like this appear underneath the code cell:\n",
    "\n",
    "```\n",
    "2019-07-29 00:54:46 Starting - Starting the training job...\n",
    "2019-07-29 00:54:47 Starting - Launching requested ML instances...\n",
    "2019-07-29 00:55:44 Starting - Preparing the instances for training......\n",
    "2019-07-29 00:56:24 Downloading - Downloading input data...\n",
    "2019-07-29 00:57:05 Training - Downloading the training image..\n",
    ".\n",
    ".\n",
    ".\n",
    "2019-07-29 00:57:31 Uploading - Uploading generated training model\n",
    "2019-07-29 00:57:31 Completed - Training job completed\n",
    "Billable seconds: 68\n",
    "CPU times: user 1.78 s, sys: 18.7 ms, total: 1.8 s\n",
    "Wall time: 3min 13s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-29 00:54:46 Starting - Starting the training job...\n",
      "2019-07-29 00:54:47 Starting - Launching requested ML instances...\n",
      "2019-07-29 00:55:44 Starting - Preparing the instances for training......\n",
      "2019-07-29 00:56:24 Downloading - Downloading input data...\n",
      "2019-07-29 00:57:05 Training - Downloading the training image..\n",
      "\u001b[31mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:21 INFO 140106530510656] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/resources/default-input.json: {u'_enable_profiler': u'false', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'local_lloyd_num_trials': u'auto', u'_log_level': u'info', u'_kvstore': u'auto', u'local_lloyd_init_method': u'kmeans++', u'force_dense': u'true', u'epochs': u'1', u'init_method': u'random', u'local_lloyd_tol': u'0.0001', u'local_lloyd_max_iter': u'300', u'_disable_wait_to_read': u'false', u'extra_center_factor': u'auto', u'eval_metrics': u'[\"msd\"]', u'_num_kv_servers': u'1', u'mini_batch_size': u'5000', u'half_life_time_size': u'0', u'_num_slices': u'1'}\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:21 INFO 140106530510656] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'feature_dim': u'2', u'k': u'10', u'force_dense': u'True'}\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:21 INFO 140106530510656] Final configuration: {u'_tuning_objective_metric': u'', u'extra_center_factor': u'auto', u'local_lloyd_init_method': u'kmeans++', u'force_dense': u'True', u'epochs': u'1', u'feature_dim': u'2', u'local_lloyd_tol': u'0.0001', u'_disable_wait_to_read': u'false', u'eval_metrics': u'[\"msd\"]', u'_num_kv_servers': u'1', u'mini_batch_size': u'5000', u'_enable_profiler': u'false', u'_num_gpus': u'auto', u'local_lloyd_num_trials': u'auto', u'_log_level': u'info', u'init_method': u'random', u'half_life_time_size': u'0', u'local_lloyd_max_iter': u'300', u'_kvstore': u'auto', u'k': u'10', u'_num_slices': u'1'}\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:21 WARNING 140106530510656] Loggers have already been setup.\u001b[0m\n",
      "\u001b[31mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:21 INFO 140106530510656] Using default worker.\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:21 INFO 140106530510656] Loaded iterator creator application/x-recordio-protobuf for content type ('application/x-recordio-protobuf', '1.0')\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:21 INFO 140106530510656] Create Store: local\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:21 INFO 140106530510656] nvidia-smi took: 0.0251741409302 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:21 INFO 140106530510656] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:21 INFO 140106530510656] Setting up with params: {u'_tuning_objective_metric': u'', u'extra_center_factor': u'auto', u'local_lloyd_init_method': u'kmeans++', u'force_dense': u'True', u'epochs': u'1', u'feature_dim': u'2', u'local_lloyd_tol': u'0.0001', u'_disable_wait_to_read': u'false', u'eval_metrics': u'[\"msd\"]', u'_num_kv_servers': u'1', u'mini_batch_size': u'5000', u'_enable_profiler': u'false', u'_num_gpus': u'auto', u'local_lloyd_num_trials': u'auto', u'_log_level': u'info', u'init_method': u'random', u'half_life_time_size': u'0', u'local_lloyd_max_iter': u'300', u'_kvstore': u'auto', u'k': u'10', u'_num_slices': u'1'}\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:21 INFO 140106530510656] 'extra_center_factor' was set to 'auto', evaluated to 10.\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:21 INFO 140106530510656] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:21 INFO 140106530510656] number of center slices 1\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 5000, \"sum\": 5000.0, \"min\": 5000}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}, \"Total Records Seen\": {\"count\": 1, \"max\": 5000, \"sum\": 5000.0, \"min\": 5000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 5000, \"sum\": 5000.0, \"min\": 5000}, \"Reset Count\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}}, \"EndTime\": 1564361841.993677, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"init_train_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/KMeansWebscale\"}, \"StartTime\": 1564361841.993646}\n",
      "\u001b[0m\n",
      "\u001b[31m[2019-07-29 00:57:21.998] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 48, \"num_examples\": 1, \"num_bytes\": 160000}\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:22 INFO 140106530510656] Iter 10: Short term msd 46.405341. Long term msd 50.694177\u001b[0m\n",
      "\u001b[31m[2019-07-29 00:57:22.253] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 1, \"duration\": 254, \"num_examples\": 18, \"num_bytes\": 2789888}\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:22 INFO 140106530510656] processed a total of 87184 examples\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:22 INFO 140106530510656] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 18, \"sum\": 18.0, \"min\": 18}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 18, \"sum\": 18.0, \"min\": 18}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 87184, \"sum\": 87184.0, \"min\": 87184}, \"Total Batches Seen\": {\"count\": 1, \"max\": 19, \"sum\": 19.0, \"min\": 19}, \"Total Records Seen\": {\"count\": 1, \"max\": 92184, \"sum\": 92184.0, \"min\": 92184}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 87184, \"sum\": 87184.0, \"min\": 87184}, \"Reset Count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1564361842.253606, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/KMeansWebscale\", \"epoch\": 0}, \"StartTime\": 1564361841.998304}\n",
      "\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:22 INFO 140106530510656] #throughput_metric: host=algo-1, train throughput=341305.295323 records/second\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:22 WARNING 140106530510656] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:22 INFO 140106530510656] shrinking 100 centers into 10\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:22 INFO 140106530510656] local kmeans attempt #0. Current mean square distance 42.058079\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:22 INFO 140106530510656] local kmeans attempt #1. Current mean square distance 47.231346\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:22 INFO 140106530510656] local kmeans attempt #2. Current mean square distance 42.428646\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:22 INFO 140106530510656] local kmeans attempt #3. Current mean square distance 49.768463\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:22 INFO 140106530510656] local kmeans attempt #4. Current mean square distance 38.967083\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:22 INFO 140106530510656] local kmeans attempt #5. Current mean square distance 39.077560\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:22 INFO 140106530510656] local kmeans attempt #6. Current mean square distance 38.824406\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:22 INFO 140106530510656] local kmeans attempt #7. Current mean square distance 40.465004\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:22 INFO 140106530510656] local kmeans attempt #8. Current mean square distance 40.437111\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:22 INFO 140106530510656] local kmeans attempt #9. Current mean square distance 42.829659\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:22 INFO 140106530510656] finished shrinking process. Mean Square Distance = 39\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:22 INFO 140106530510656] #quality_metric: host=algo-1, train msd <loss>=38.8244056702\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:22 INFO 140106530510656] predict compute msd took: 30.2583%, (0.078261 secs)\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:22 INFO 140106530510656] compute all data-center distances: inner product took: 22.9514%, (0.059362 secs)\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:22 INFO 140106530510656] gradient: cluster size  took: 18.7406%, (0.048471 secs)\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:22 INFO 140106530510656] gradient: cluster center took: 6.6421%, (0.017179 secs)\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:22 INFO 140106530510656] batch data loading with context took: 4.0785%, (0.010549 secs)\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:22 INFO 140106530510656] collect from kv store took: 3.9665%, (0.010259 secs)\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:22 INFO 140106530510656] splitting centers key-value pair took: 3.2907%, (0.008511 secs)\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:22 INFO 140106530510656] compute all data-center distances: point norm took: 3.2720%, (0.008463 secs)\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:22 INFO 140106530510656] gradient: one_hot took: 2.8511%, (0.007374 secs)\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:22 INFO 140106530510656] update state and report convergance took: 2.8434%, (0.007354 secs)\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:22 INFO 140106530510656] compute all data-center distances: center norm took: 0.8898%, (0.002301 secs)\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:22 INFO 140106530510656] predict minus dist took: 0.1355%, (0.000350 secs)\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:22 INFO 140106530510656] update set-up time took: 0.0800%, (0.000207 secs)\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:22 INFO 140106530510656] TOTAL took: 0.258642673492\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:22 INFO 140106530510656] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"finalize.time\": {\"count\": 1, \"max\": 300.4438877105713, \"sum\": 300.4438877105713, \"min\": 300.4438877105713}, \"initialize.time\": {\"count\": 1, \"max\": 37.05096244812012, \"sum\": 37.05096244812012, \"min\": 37.05096244812012}, \"model.serialize.time\": {\"count\": 1, \"max\": 0.1380443572998047, \"sum\": 0.1380443572998047, \"min\": 0.1380443572998047}, \"update.time\": {\"count\": 1, \"max\": 255.11598587036133, \"sum\": 255.11598587036133, \"min\": 255.11598587036133}, \"epochs\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}, \"state.serialize.time\": {\"count\": 1, \"max\": 1.4908313751220703, \"sum\": 1.4908313751220703, \"min\": 1.4908313751220703}, \"_shrink.time\": {\"count\": 1, \"max\": 298.7639904022217, \"sum\": 298.7639904022217, \"min\": 298.7639904022217}}, \"EndTime\": 1564361842.556131, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/KMeansWebscale\"}, \"StartTime\": 1564361841.949236}\n",
      "\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:22 INFO 140106530510656] Test data is not provided.\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 671.81396484375, \"sum\": 671.81396484375, \"min\": 671.81396484375}, \"setuptime\": {\"count\": 1, \"max\": 14.061927795410156, \"sum\": 14.061927795410156, \"min\": 14.061927795410156}}, \"EndTime\": 1564361842.556472, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/KMeansWebscale\"}, \"StartTime\": 1564361842.55622}\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2019-07-29 00:57:31 Uploading - Uploading generated training model\n",
      "2019-07-29 00:57:31 Completed - Training job completed\n",
      "Billable seconds: 68\n",
      "CPU times: user 1.78 s, sys: 18.7 ms, total: 1.8 s\n",
      "Wall time: 3min 13s\n"
     ]
    }
   ],
   "source": [
    "# Create a training job and time it. Running this code cell will send a training job request to AWS Sagemaker\n",
    "%%time\n",
    "kmeans.fit(kmeans.record_set(data_train), job_name= job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congratulations** on building and training a model on the cloud using unsupervised machine learning algorithm and saving it! Next we are going to deserialise the model so that we can use its output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Model Deserialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deserialise the compressed model output saved on our S3 bucket we need to import the following packages.\n",
    "\n",
    "- **Boto** is the Amazon Web Services (AWS) SDK for Python. It enables Python developers to create, configure, and manage AWS services, such as EC2 and S3. Boto provides an easy to use, object-oriented API, as well as low-level access to AWS service. See [Documentation](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html)\n",
    "\n",
    "> **Exercise**: Import `boto` package, then use the AWS Python SDK boto3 to download the compressed model output from the S3 bucket to a file. You will need to construct a url to the model output and save it to `path_to_model` variable. Then pass `path_to_model` to the following command `boto3.resource(\"s3\").Bucket(bucket).download_file(path_to_model, file_name_to_save_to)`. - See [boto3 Documentation](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html?highlight=s3.object#S3.Client.download_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required packages for deserialisation\n",
    "import boto3\n",
    "\n",
    "# Construct a url to the model output. Compressed model outputs are saved \n",
    "# in the job_name folder under the model-artifacts folder\n",
    "path_to_model = \"model-artifacts/\" + job_name + \"/output/model.tar.gz\"\n",
    "\n",
    "# Use the AWS Python SDK boto3 to download the compressed model output from S3 bucket onto `model.tar.gz` file.\n",
    "boto3.resource(\"s3\").Bucket(bucket).download_file(path_to_model, \"model.tar.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deserialise the compressed model output saved on our S3 bucket we need to import the following packages.\n",
    "\n",
    "- **Python's Built-in module** `os` - See [Documentation](https://docs.python.org/2/library/os.html#os.system)\n",
    "\n",
    "Python's built-in system module `os.system()` can be used to execute a shell command `tar -zxvf` on the `model.tar.gz` compressed gzipped file. The `-zxvf` flags can passed in to `os.system()` to perform the following commands: \n",
    "\n",
    "- `-z` - filters the archive through gzip\n",
    "- `-x` - extracts files from the archive\n",
    "- `-v` - verbosely lists files processed\n",
    "- `-f` - uses archive files\n",
    "\n",
    "\n",
    "See [Linux's tar Man Pages](https://linux.die.net/man/1/tar) for more details on the `tar` shell command. \n",
    "\n",
    "> **Exercise:** Use `os.system()` method to run the `tar` command on the compressed gzip file `model.tar.gz` with the above flags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required packages for deserialisation\n",
    "import os\n",
    "\n",
    "# Use Python's built-in os package to open the compressed model output\n",
    "os.system(\"tar -zxvf model.tar.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`os.system()` later can be used to execute the `unzip` shell command on `model_algo-1`. `unzip` shell command lists, tests, or extracts files from a ZIP archive. See [Linux unzip Man Pages](https://linux.die.net/man/1/unzip) for more details on the `unzip` command.\n",
    "\n",
    "> **Exercise:** Use `os.system()` method to unzip `model_algo-1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2304"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use Python's built-in os package to unzip model_algo-1 file. \n",
    "os.system(\"unzip model_algo-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the unzipped model output parameters, we need to install `mxnet` package.\n",
    "\n",
    "> **Exercise**: Use `!pip install` to install `mxnet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mxnet\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/08/186a7d67998f1e38d6d853c71c149820983c547804348f06727f552df20d/mxnet-1.5.0-py2.py3-none-manylinux1_x86_64.whl (25.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 25.4MB 1.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting graphviz<0.9.0,>=0.8.1 (from mxnet)\n",
      "  Downloading https://files.pythonhosted.org/packages/53/39/4ab213673844e0c004bed8a0781a0721a3f6bb23eb8854ee75c236428892/graphviz-0.8.4-py2.py3-none-any.whl\n",
      "Collecting numpy<2.0.0,>1.16.0 (from mxnet)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/b9/bda9781f0a74b90ebd2e046fde1196182900bd4a8e1ea503d3ffebc50e7c/numpy-1.17.0-cp36-cp36m-manylinux1_x86_64.whl (20.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 20.4MB 3.1MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from mxnet) (2.20.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.20.0->mxnet) (2019.3.9)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.20.0->mxnet) (1.23)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.20.0->mxnet) (2.6)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.20.0->mxnet) (3.0.4)\n",
      "Installing collected packages: graphviz, numpy, mxnet\n",
      "  Found existing installation: numpy 1.15.4\n",
      "    Uninstalling numpy-1.15.4:\n",
      "      Successfully uninstalled numpy-1.15.4\n",
      "Successfully installed graphviz-0.8.4 mxnet-1.5.0 numpy-1.17.0\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.2.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install mxnet package\n",
    "!pip install mxnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the model output parameters we need to import the following package:\n",
    "\n",
    "- **Package** `MXNet` \n",
    "\n",
    "> **Exercise**: Use `mxnet`'s `.ndarray.load()` method to load the model output parameters and assign it to `Kmeans_model_params` variable - See [Documentation](https://mxnet.incubator.apache.org/api/python/ndarray/ndarray.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import mxnet\n",
    "import mxnet as mx\n",
    "\n",
    "# Use mxnet to load the model parameters\n",
    "Kmeans_model_params = mx.ndarray.load(\"model_algo-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise**: Convert the model parameters to a dataframe cluster_centroids_kmeans using `pd.DataFrame()`. You can grab the model output parameters using `Kmeans_model_params[0].asnumpy()` to pass to `pd.DataFrame()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35.379860</td>\n",
       "      <td>-118.177162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41.521103</td>\n",
       "      <td>-74.812103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51.608204</td>\n",
       "      <td>0.121513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-11.612000</td>\n",
       "      <td>128.658752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>47.705780</td>\n",
       "      <td>-122.042778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>35.611134</td>\n",
       "      <td>-98.932304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>31.191694</td>\n",
       "      <td>-82.532051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>28.319733</td>\n",
       "      <td>37.477905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>41.149517</td>\n",
       "      <td>-87.080086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-18.685837</td>\n",
       "      <td>-53.455894</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    latitude   longitude\n",
       "0  35.379860 -118.177162\n",
       "1  41.521103  -74.812103\n",
       "2  51.608204    0.121513\n",
       "3 -11.612000  128.658752\n",
       "4  47.705780 -122.042778\n",
       "5  35.611134  -98.932304\n",
       "6  31.191694  -82.532051\n",
       "7  28.319733   37.477905\n",
       "8  41.149517  -87.080086\n",
       "9 -18.685837  -53.455894"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the Kmeans_model_params to a dataframe using pandas and numpy: cluster_centroids_kmeans\n",
    "cluster_centroids_kmeans = pd.DataFrame(Kmeans_model_params[0].asnumpy())\n",
    "\n",
    "# Set the column names of the cluster_centroids_kmeans dataframe to match the df_geo column names\n",
    "cluster_centroids_kmeans.columns = df_geo.columns\n",
    "\n",
    "# Print cluster_centroids_kmeans\n",
    "print(cluster_centroids_kmeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To write the content of the model output using An in-memory stream for text I/O we need to import the following package:\n",
    "\n",
    "- **Python's Built-in Package** `io` - See [Documentation](https://docs.python.org/3/library/io.html#io.StringIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '9B6F4EB4BE2499D7',\n",
       "  'HostId': 'YQU/IXFqqp4Jw7qV0U2Ijqcp3frWBpFC9THEtm3JGfCzAG9uTnAu61mvgb36mX+qKZplt+24J2U=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'YQU/IXFqqp4Jw7qV0U2Ijqcp3frWBpFC9THEtm3JGfCzAG9uTnAu61mvgb36mX+qKZplt+24J2U=',\n",
       "   'x-amz-request-id': '9B6F4EB4BE2499D7',\n",
       "   'date': 'Mon, 29 Jul 2019 01:23:38 GMT',\n",
       "   'etag': '\"2477206b3fc6b0706e3cd0fde0ca6337\"',\n",
       "   'content-length': '0',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"2477206b3fc6b0706e3cd0fde0ca6337\"'}"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Python's built-on package io\n",
    "import io\n",
    "\n",
    "# When a csv_buffer object is created, it is initialized using StringIO() constructor\n",
    "# Here no string is given to the StringIO() so the csv_buffer object is empty.\n",
    "csv_buffer = io.StringIO()\n",
    "\n",
    "# Use pandas .to_csv() method to weite the cluster_centroids_kmeans dataframe to a csv file\n",
    "cluster_centroids_kmeans.to_csv(csv_buffer, index = False)\n",
    "\n",
    "# Use Amazon's boto3 package to create an S3 resource\n",
    "s3_resource = boto3.resource(\"s3\")\n",
    "\n",
    "# Create an S3 object at a path given, using the .Object() method in the given `bucket`.\n",
    "# Then save the content of the csv_buffer file onto the newly created S3 object using the .put() methods\n",
    "s3_resource.Object(bucket, \"results/ten_locations_kmeans.csv\").put(Body = csv_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONGRATULATIONS!!!\n",
    "Well done on compeleting this difficult part of the assignment. All is now left for you to do is to visualise the model outputs you have saved in the `ten_locations_kmeans.csv` file in your S3 bucket on a map. Simply create a **AWS Quicksight** account and use the `my-manifest.json` file under the `quicksight` folder to configure AWS Quicksight.\n",
    "\n",
    "Again, Well done on compeleting the above assignments! This was a hard exercise and you have learned a lot about how to use AWS Sagemaker to train an unsupervised machine learning model in the cloud. We hope that you enjoyed this **Introduction to unsupervised machine learning with AWS** Workshop. To learn more about AWS Sagemaker and machine learning in the cloud check out a few resources we have provided in our repo's [README.md](https://github.com/beginners-machine-learning-london/intro_to_unsupervised_ml_with_AWS_Sagemaker).\n",
    "\n",
    "Also make sure to sign up on our meetup group to be informed of future workshops! [London Beginners Machine Learnign Meetup](https://www.meetup.com/beginners-machine-learning-london/).\n",
    "\n",
    "And join our [slack channel](https://join.slack.com/t/beginnersmach-wlf5812/shared_invite/enQtNzAzODA4OTY3MTcyLWU2ZDMzNGU2YTQ4ZDk5ZjY3OTk1YWU2OGU5NWRmMjM1NzkwM2MwYjk5MDNhZWE1YWVmNzY1MjgzZDk4OGE1OGE) too to ask questions, discuss ML with other BML community members and suggest us topics for future workshops."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
