{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Ali Parandeh - Beginners Machine Learning - London\n",
    "\n",
    "# Introduction to Unsupervised Machine Learning with AWS Sagemaker\n",
    "In this interesting 3hr workshop, you will take the massive dataset of UFO sightings (80,000 reports over the past century) from [National UFO Reporting Center (NUFORC)](http://www.nuforc.org/) and use Amazon's machine learning services ([AWS Sagemaker](https://aws.amazon.com/sagemaker/)) to identify the top 10 locations that are most likely to have UFO sightings. To do so, you will need to use an unsupervised machine learning algorithm.\n",
    "\n",
    "You will then take your trained model, deserialise it, convert its output to a csv format and visualise it on a map using AWS [Quicksight](https://aws.amazon.com/quicksight/) to see where these locations are. Then you can try correlating these locations with landmarks.\n",
    "\n",
    "The general machine learning workflow with AWS Sagemaker is shown below. For this assignment we will not evaluate or deploy the model but only use its output to visualise the results on a world map.\n",
    "\n",
    "<img src=\"https://docs.aws.amazon.com/sagemaker/latest/dg/images/ml-concepts-10.png\">\n",
    "\n",
    "### What is Unsupervised Machine Learning? \n",
    "\n",
    "With unsupervised learning, data features are fed into the learning algorithm, which determines how to label them (usually with numbers 0,1,2..) and based on what. This “based on what” part dictates which unsupervised learning algorithm to follow.\n",
    "\n",
    "Most unsupervised learning-based applications utilize the sub-field called **Clustering**. \n",
    "\n",
    "One of the most famous topics under the realm of Unsupervised Learning in Machine Learning is k-Means Clustering. Even though this clustering algorithm is fairly simple, it can look challenging to newcomers into the field. \n",
    "\n",
    "### What is the difference between supervised and unsupervised machine learning?\n",
    "\n",
    "The main difference between Supervised and Unsupervised learning algorithms is the absence of data labels in the latter.\n",
    "\n",
    "### What does clustering mean?\n",
    "\n",
    "**Clustering** is the process of grouping data samples together into clusters based on a certain feature that they share — exactly the purpose of unsupervised learning in the first place.\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*tWaaZX75oumVwBMcKN-eHA.png\">\n",
    "\n",
    "Source: [Clustering using K-means algorithm](https://towardsdatascience.com/clustering-using-k-means-algorithm-81da00f156f6)\n",
    "\n",
    "### How does the K-Means Algorithm work?\n",
    "\n",
    "Being a clustering algorithm, **k-Means** takes data points as input and groups them into `k` clusters. This process of grouping is the training phase of the learning algorithm. The result would be a model that takes a data sample as input and returns the cluster that the new data point belongs to, according the training that the model went through.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/700/1*6EOTS1IE2ULWC9SKgf7mYw.png\">\n",
    "\n",
    "Source - [How Does k-Means Clustering in Machine Learning Work?](https://towardsdatascience.com/how-does-k-means-clustering-in-machine-learning-work-fdaaaf5acfa0)\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/700/1*4LOxZL6bFl3rXlr2uCiKlQ.gif\">\n",
    "\n",
    "Source: [How Does k-Means Clustering in Machine Learning Work?](https://towardsdatascience.com/how-does-k-means-clustering-in-machine-learning-work-fdaaaf5acfa0)\n",
    "\n",
    "Check out the the two articles below to learn more about how the K-Means Algorithm work:\n",
    "\n",
    "- [Clustering using K-means algorithm](https://towardsdatascience.com/clustering-using-k-means-algorithm-81da00f156f6)\n",
    "- [How Does k-Means Clustering in Machine Learning Work?](https://towardsdatascience.com/how-does-k-means-clustering-in-machine-learning-work-fdaaaf5acfa0)\n",
    "\n",
    "\n",
    "### Where can you use k-means?\n",
    "\n",
    "The **k-means algorithm** can be a good fit for finding patterns or groups in large datasets that have not been explicitly labeled. Here are some example use cases in different domains:\n",
    "\n",
    "**E-commerce**\n",
    "\n",
    "- Classifying customers by purchase history or clickstream activity.\n",
    "\n",
    "**Healthcare**\n",
    "\n",
    "- Detecting patterns for diseases or success treatment scenarios.\n",
    "- Grouping similar images for image detection.\n",
    "\n",
    "**Finance**\n",
    "\n",
    "- Detecting fraud by detecting anomalies in the dataset. For example, detecting credit card frauds by abnormal purchase patterns.\n",
    "\n",
    "**Technology**\n",
    "\n",
    "- Building a network intrusion detection system that aims to identify attacks or malicious activity.\n",
    "\n",
    "**Meteorology**\n",
    "\n",
    "- Detecting anomalies in sensor data collection such as storm forecasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Importing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this part of the assignment, we need to import the following packages: \n",
    "\n",
    "- **Amazon SageMaker Python SDK**: Amazon SageMaker Python SDK is an open source library for training and deploying machine-learned models on Amazon SageMaker. See [Documentation](https://sagemaker.readthedocs.io/en/stable/index.html)\n",
    "- **Python Built-in Library** [datetime](https://docs.python.org/2/library/datetime.html)\n",
    "- **Numpy** and **Pandas**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import the above packages below\n",
    "import ____ as pd\n",
    "import ____ as np\n",
    "import ____\n",
    "from ____ import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise:** Construct a url to the the dataset location in your S3 bucket using the following expression and save it to `data_location`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Construct the url path to your dataset file that you have just uploaded to your newly created S3 bucket\n",
    "bucket = \"____\"\n",
    "prefix = \"ufo_dataset\"\n",
    "data_key = \"ufo_complete.csv\"\n",
    "\n",
    "# Construct a url string and save it to data_location variable\n",
    "data_location = \"s3://{}/{}/{}\".format(bucket, prefix, data_key)\n",
    "\n",
    "# print data_location\n",
    "print(data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Internally do not process the file in chunks when loading the csv onto a dataframe \n",
    "# to ensure avoid mixed type inferences when importing the large UFO dataset. \n",
    "df = pd.read_csv(data_location, low_memory= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the tail of the dataframe\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Inspect the shape of the dataframe\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Clearning, transforming and preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Select the 'latitude' and 'longitude' columns and save it as a new dataframe `df_geo` with .copy().\n",
    "df_geo = df[[\"____\", \"____\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the tail of df_geo\n",
    "df_geo.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fully inspect the df_geo dataframe\n",
    "df_geo.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon successfull inspection of the above dataframe, you should note the following with this dataframe:\n",
    "\n",
    "- There are no `null` or missing values in both columns. However, we still need to check for other incorrect entries that are not **coordinates**. Examples include: `0`, `string`s, etc.\n",
    "- The `latitude` column has a `dtype` of `object`. This means the column may have missing or string values where the rest of the values are numbers. If the entries in the column are non-homogenous, pandas will store the column as a `string` or `object` data type. To clean the data in this column we can use pandas' `pd.to_numeric()` method to convert the data in this column to `float` for processing. The machine learning algorithm expects the data passed in to it to be numerical digits `float`s or `int`s not `string`s. - See [Documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_numeric.html) on how to use this method.\n",
    "\n",
    "> **Exercise:** Convert the `latitude` column's datatype to `float`. You can pass in the `errors = \"coerce\"` option to `pd.to_numeric()` method to enforce the conversion. When conversion is not possible - i.e. values are `string`s - these strings will be replaced with `NaNs`. Therefore, you need to use a `.dropna()` method to drop rows where `NaNs` exist. Then check whether the column formats have been converted to numerical data types `float` and if any missing values are still present. **Note**: You can pass in `inplace = True` argument to `.dropna()` methods so that operations are performed in place and to avoid re-assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: Convert the column values to numeric and whenever this is not possible replace the value with NaNs\n",
    "df_geo[\"latitude\"] = pd.____(df_geo.____, errors = \"____\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of null values in the dataframe - Expecting this to be non-zero\n",
    "print(\"Number of null values in the dataframe before dropping rows is {}\".format(df_geo.isnull().any().sum()))\n",
    "\n",
    "# TODO: Drop all rows that NaN Values\n",
    "df_geo.____(inplace=____)\n",
    "\n",
    "# Count the number of null values in the dataframe - Expecting this to be zero\n",
    "print(\"Number of null values in the dataframe before dropping rows is {}\". format(df_geo.isnull().any().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many rows in the df have 0 values\n",
    "print(df_geo[(df_geo.longitude == 0) | (df_geo.latitude == 0) ].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Select all rows that have non-zero coordinate values and re-assign the selection to df_geo\n",
    "df_geo = df_geo[(df_geo.longitude != ____) &(df_geo.latitude != ____) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the there are no coordinate values in the df_geo dataframe with 0\n",
    "print(df_geo[(df_geo.longitude == 0) &(df_geo.latitude == 0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Re-checking the dataframe to ensure both columns have numerical datatype such as `float` or `int`.\n",
    "df_geo.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check if we have any missing values (NaNs) in our dataframe\n",
    "missing_values = df_geo.isnull().values.any()\n",
    "print(\"Are there any missing values? {}\".format(missing_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there are any missing values in the dataframe, show them\n",
    "if (missing_values):\n",
    "    df_geo[df_geo.isnull().any(axis = 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: store the cleaned up dataframe column values as a 2D numpy array (matrix) with datatype of float32\n",
    "data_train = df_geo.values.astype(\"____\")\n",
    "\n",
    "# Print the 2D numpy array\n",
    "data_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Visualising the last 5000 reports of the data on the map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the useful packages for visualising the data on a map is called **plotly**. \n",
    "\n",
    "We can import the following module from plotly package as `px`:\n",
    "\n",
    "- **plotly**'s [express](https://plot.ly/python/plotly-express/) - Plotly Express is a terse, consistent, high-level wrapper around `plotly.graph_objects` for rapid data exploration and figure generation.\n",
    "\n",
    "For data available as a tidy pandas DataFrame, we can use the Plotly Express function `px.scatter_geo` for a geographical scatter plot. The `color` argument is used to set the color of the markers from a given column of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Showing only the last 5000 rows only on a map\n",
    "fig = px.scatter_geo(df_geo.iloc[-5000: -1, :], lat=\"latitude\", lon = \"longitude\", \n",
    "                     title=\"UFO Reports by Latitude/Longitude in the world - Last 5000 Reports\", color = \"longitude\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that most of the recent 5000 UFO reports have been located in the United States. Let's take a closer look at United States by using `plotly`'s `geo` layout feature to show sightings on the US map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.offline import iplot\n",
    "\n",
    "data = [dict(\n",
    "        type = 'scattergeo',\n",
    "        locationmode = 'USA-states',\n",
    "        lat = df_geo.iloc[-5000:-1, 0],\n",
    "        lon = df_geo.iloc[-5000:-1, 1],\n",
    "        mode = 'markers',\n",
    "        marker = dict(\n",
    "            size = 5.5,\n",
    "            opacity = 0.75,\n",
    "            color = 'rgb(0, 163, 81)',\n",
    "            line = dict(color = 'rgb(255, 255, 255)', width = 1))\n",
    "        )]\n",
    "\n",
    "layout = dict(\n",
    "         title = 'UFO Reports by Latitude/Longitude in United States - Last 5000 Reports',\n",
    "         geo = dict(\n",
    "             scope = 'usa',\n",
    "             projection = dict(type = 'albers usa'),\n",
    "             showland = True,\n",
    "             landcolor = 'rgb(250, 250, 250)',\n",
    "             subunitwidth = 1,\n",
    "             subunitcolor = 'rgb(217, 217, 217)',\n",
    "             countrywidth = 1,\n",
    "             countrycolor = 'rgb(217, 217, 217)',\n",
    "             showlakes = True,\n",
    "             lakecolor = 'rgb(255, 255, 255)')\n",
    "        )\n",
    "\n",
    "figure = dict(data = data, layout = layout)\n",
    "iplot(figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create and train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of clusters and output location URL to save the trained model\n",
    "num_clusters = 10\n",
    "output_location = \"s3://\" + bucket + \"/model-artifacts\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To pass a training command to Amazon Sagemaker, we need to grab the details of the current execution role **ARN ID** whose credentials we are using to call the Sagemaker API. \n",
    "\n",
    "> **Exercise:** Grab the ARN ID of your current Execution role using the `sagemaker` SDK - See [Documentation](https://sagemaker.readthedocs.io/en/stable/session.html?highlight=get%20execution#sagemaker.session.get_execution_role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Get the execution role ARN ID to pass to the sagemaker API later on\n",
    "role = sagemaker.____()\n",
    "\n",
    "# Check that you have this step correctly performed\n",
    "print(role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now can use Amazon's built-in K-means ML algorithm to find `k` clusters of data in our unlabeled UFO dataset.\n",
    "\n",
    "Amazon SageMaker uses a modified version of the web-scale k-means clustering algorithm. Compared with the original version of the algorithm, the version used by Amazon SageMaker is more accurate. Like the original algorithm, it scales to massive datasets and delivers improvements in training time. To do this, the version used by Amazon SageMaker streams mini-batches (small, random subsets) of the training data. The k-means algorithm expects tabular data, where rows represent the observations that you want to cluster, and the columns represent attributes of the observations. See [Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/k-means.html)\n",
    "\n",
    "To ask AWS sagemaker for training a model using this algorithm we need to define a **K-means Estimator**. KMeans estimators can be configured by setting **hyperparameters**. These hyperparameters are arguments passed into the estimator's Constructor Function. \n",
    "\n",
    "This estimator requires the following hyperparameters to be passed in `sagemaker.KMeans()`:\n",
    "\n",
    "- `role` (str) – An AWS IAM role (either name or full ARN)\n",
    "- `train_instance_count` (int) – Number of Amazon EC2 instances to use for training. We only need 1 for this exercise.\n",
    "- `train_instance_type` (str) – Type of EC2 instance to use for training, for example, ‘ml.c4.xlarge’. This is the **compute resources** that you want Amazon SageMaker to use for model training. Compute resources are ML compute instances that are managed by Amazon SageMaker.\n",
    "- `k` (int) – The number of clusters to produce. We need to 10 for this exercise.\n",
    "- `output_path` (str) - The URL of the S3 bucket where you want to store the output of the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define the training API request to AWS Sagemaker\n",
    "kmeans = sagemaker.____(role = ____,\n",
    "               train_instance_count = ____,\n",
    "               train_instance_type = \"____\",\n",
    "               output_path = ____,\n",
    "               k = ____)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following diagram shows how you train and deploy a model with Amazon SageMakern - See [Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-training.html) For this assignment, Amazon SageMaker provides training algorithms that are great out-of-the-box solution for quick model training.  We have used some helper code above to clean and prepare the dataset and configure AWS Sagemaker API calls but do not need to specify training code or even a training code image from EC2 Container Registry. We only need to pass in the dataset for training with AWS's KMeans default algorithm. If we wanted to specify our own algorithms or use one of the popular deep learning frameworks - tensorflow/etc. - then we provide additional training code.\n",
    "\n",
    "<img src=\"https://docs.aws.amazon.com/sagemaker/latest/dg/images/sagemaker-architecture-training-2.png\">\n",
    "\n",
    "To train a model in Amazon SageMaker, you create a **training job** using the `kmeans.fit()` method. - See [Documentation](https://sagemaker.readthedocs.io/en/stable/kmeans.html?highlight=kmeans.fit#sagemaker.KMeans.fit)\n",
    "\n",
    "The training job requires the following information passed in to `.fit()` method:\n",
    "\n",
    "- `record_set(data_train)` - The training records to train the KMeans Estimator on. Here `data_train` must be passed in to the `kmeans.record_set()` method to convert our 2D numpy array data to a `RecordSet` object that is required by the algorithm. - See [Documentation](https://sagemaker.readthedocs.io/en/stable/sagemaker.amazon.amazon_estimator.html?highlight=record_set()#sagemaker.amazon.amazon_estimator.AmazonAlgorithmEstimatorBase.record_set)\n",
    "- `job_name` (str) - Training job name. If not specified, the estimator generates a default job name, based on the training image name and current timestamp.\n",
    "\n",
    "Amazon SageMaker then launches the ML compute instances and uses the training dataset to train the model. It saves the resulting model artifacts and other output in the S3 bucket you specified for that purpose.\n",
    "\n",
    "Here we are going to construct a job name using the following expression and Python's built-in `datetime` module. This ensures our `job_name` is unique. Each training job requires a **unique** `job_name`. Otherwise, AWS will throw an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Construct a unique job_name using datetime module\n",
    "job_name = \"kmeans-geo-job-{}\".format(datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
    "\n",
    "# Print job_name\n",
    "print(\"Here is the job name: {}\".format(job_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise**: Create a training job using `kmeans.fit()`. Use the AWS documentation links above to figure out how to pass in the arguments to `kmeans.fit()` for the training job to commence. \n",
    "\n",
    "If you do this step right, you should see outputs like this appear underneath the code cell:\n",
    "\n",
    "```\n",
    "2019-07-29 00:54:46 Starting - Starting the training job...\n",
    "2019-07-29 00:54:47 Starting - Launching requested ML instances...\n",
    "2019-07-29 00:55:44 Starting - Preparing the instances for training......\n",
    "2019-07-29 00:56:24 Downloading - Downloading input data...\n",
    "2019-07-29 00:57:05 Training - Downloading the training image..\n",
    ".\n",
    ".\n",
    ".\n",
    "2019-07-29 00:57:31 Uploading - Uploading generated training model\n",
    "2019-07-29 00:57:31 Completed - Training job completed\n",
    "Billable seconds: 68\n",
    "CPU times: user 1.78 s, sys: 18.7 ms, total: 1.8 s\n",
    "Wall time: 3min 13s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# TOOD: Create a training job and time it. Running this code cell will send a training job request to AWS Sagemaker\n",
    "kmeans.fit(kmeans.record_set(_____), job_name= _____)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congratulations** on building and training a model on the cloud using unsupervised machine learning algorithm and saving it! Next we are going to deserialise the model so that we can use its output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Model Deserialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deserialise the compressed model output saved on our S3 bucket we need to import the following packages.\n",
    "\n",
    "- **Boto** is the Amazon Web Services (AWS) SDK for Python. It enables Python developers to create, configure, and manage AWS services, such as EC2 and S3. Boto provides an easy to use, object-oriented API, as well as low-level access to AWS service. See [Documentation](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html)\n",
    "\n",
    "> **Exercise**: Import `boto3` package, then use the AWS Python SDK boto3 to download the compressed model from the S3 bucket to a file. You will need to construct a url to the model and save it to `path_to_model` variable. Then pass `path_to_model` to the following command `boto3.resource(\"s3\").Bucket(bucket).download_file(path_to_model, file_name_to_save_to)`. - See [boto3 Documentation](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html?highlight=s3.object#S3.Client.download_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import boto3\n",
    "import ____\n",
    "\n",
    "# Construct a url to the model. Compressed model is saved under the model-artifacts folder\n",
    "path_to_model = \"model-artifacts/\" + job_name + \"/output/model.tar.gz\"\n",
    "\n",
    "# TODO: Use the AWS Python SDK boto3 to download the compressed model output from S3 bucket onto `model.tar.gz` file.\n",
    "boto3.____(\"____\").____(____).download_file(____, \"model.tar.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deserialise the compressed model output saved on our S3 bucket we need to import the following packages.\n",
    "\n",
    "- **Python's Built-in module** `os` - See [Documentation](https://docs.python.org/2/library/os.html#os.system)\n",
    "\n",
    "Python's built-in system module `os.system()` can be used to execute a shell command `tar -zxvf` on the `model.tar.gz` compressed gzipped file. This command shell helps to extract tar files out a `tar.gz` archives. The `-zxvf` flags can passed in to `os.system()` to perform the following commands: \n",
    "\n",
    "- `-z` - The file is a “gzipped” file\n",
    "- `-x` - Extract files\n",
    "- `-v` - Verbose, print the file names as they are extracted one by one\n",
    "- `-f` - Use the following tar archive for the operation\n",
    "\n",
    "\n",
    "See [Linux's tar Man Pages](https://linux.die.net/man/1/tar) for more details on the `tar` shell command. \n",
    "\n",
    "> **Exercise:** Use `os.system()` method to run the `tar` command on the compressed gzip file `model.tar.gz` with the above flags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import the required packages for deserialisation\n",
    "import os\n",
    "\n",
    "# TODO: Use Python's built-in os package to open the compressed model output\n",
    "os.system(\"____ -____ model.tar.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`os.system()` later can be used to execute the `unzip` shell command on `model_algo-1`. `unzip` shell command lists, tests, or extracts files from a ZIP archive. See [Linux unzip Man Pages](https://linux.die.net/man/1/unzip) for more details on the `unzip` command.\n",
    "\n",
    "> **Exercise:** Use `os.system()` method to unzip `model_algo-1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use Python's built-in os package to unzip model_algo-1 file. \n",
    "os.system(\"____ model_algo-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the unzipped model output parameters, we need to install `mxnet` package.\n",
    "\n",
    "> **Exercise**: Use `!pip install` to install `mxnet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO: Install mxnet package\n",
    "!pip install ____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the model output parameters we need to import the following package:\n",
    "\n",
    "- **MXNet**: A flexible and efficient library for deep learning. - See [Documentation](https://mxnet.apache.org/versions/master/api/python/index.html) \n",
    "\n",
    "> **Exercise**: Use `mxnet`'s `.ndarray.load()` method to load the model output parameters and assign it to `Kmeans_model_params` variable - See [Documentation](https://mxnet.incubator.apache.org/api/python/ndarray/ndarray.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import mxnet\n",
    "import ____ as mx\n",
    "\n",
    "# TODO: Use mxnet to load the model parameters\n",
    "Kmeans_model_params = mx.____.____(\"model_algo-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise**: Convert the model parameters to a dataframe called `cluster_centroids_kmeans` using `pd.DataFrame()`. You can grab the model output parameters using `Kmeans_model_params[0].asnumpy()` to pass to `pd.DataFrame()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Convert the Kmeans_model_params to a dataframe using pandas and numpy: cluster_centroids_kmeans\n",
    "cluster_centroids_kmeans = pd.____(____[0].____())\n",
    "\n",
    "# TODO: Set the column names of the cluster_centroids_kmeans dataframe to match the df_geo column names\n",
    "cluster_centroids_kmeans.____ = df_geo.____\n",
    "\n",
    "# Print cluster_centroids_kmeans\n",
    "print(cluster_centroids_kmeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To write the content of the model output using An in-memory stream for text I/O we need to import the following package:\n",
    "\n",
    "- **Python's Built-in Package** `io` - See [Documentation](https://docs.python.org/3/library/io.html#io.StringIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import Python's built-on package io\n",
    "import ____\n",
    "\n",
    "# When a csv_buffer object is created, it is initialized using StringIO() constructor\n",
    "# Here no string is given to the StringIO() so the csv_buffer object is empty.\n",
    "csv_buffer = io.StringIO()\n",
    "\n",
    "# TODO: Use pandas .to_csv() method to weite the cluster_centroids_kmeans dataframe to a csv file\n",
    "cluster_centroids_kmeans.____(csv_buffer, index = False)\n",
    "\n",
    "# TODO: Let's use Amazon S3\n",
    "s3_resource = boto3.resource(\"____\")\n",
    "\n",
    "# Use the .Object() method to upload an object in the given `bucket`\n",
    "# Save the content of the csv_buffer file using the .put() method\n",
    "s3_resource.Object(bucket, \"results/ten_locations_kmeans.csv\").put(Body = csv_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly visualise where these top 10 coordinates are! We will use **AWS Quicksights** later on to for reporting these locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualise the top 10 locations in the world most likely to have UFO Sightings\n",
    "fig = px.scatter_geo(cluster_centroids_kmeans, lat=\"____\", lon = \"____\", \n",
    "                     title=\"Top 10 Locations in the world mostly likely to have UFO Sightings\", color = \"longitude\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualise the top locations in the US most likely to have UFO Sightings\n",
    "data = [dict(\n",
    "        type = '____',\n",
    "        locationmode = 'USA-states',\n",
    "        lat = ____.iloc[:, 0],\n",
    "        lon = ____.iloc[:, 1],\n",
    "        mode = 'markers',\n",
    "        marker = dict(\n",
    "            size = 5.5,\n",
    "            opacity = 0.75,\n",
    "            color = 'rgb(0, 163, 81)',\n",
    "            line = dict(color = 'rgb(255, 255, 255)', width = 1))\n",
    "        )]\n",
    "\n",
    "layout = dict(\n",
    "         title = 'Top locations in the United States most likely to have UFO Sightings',\n",
    "         geo = dict(\n",
    "             scope = '____',\n",
    "             projection = dict(type = 'albers usa'),\n",
    "             showland = True,\n",
    "             landcolor = 'rgb(250, 250, 250)',\n",
    "             subunitwidth = 1,\n",
    "             subunitcolor = 'rgb(217, 217, 217)',\n",
    "             countrywidth = 1,\n",
    "             countrycolor = 'rgb(217, 217, 217)',\n",
    "             showlakes = True,\n",
    "             lakecolor = 'rgb(255, 255, 255)')\n",
    "        )\n",
    "\n",
    "figure = dict(data = ____, layout = ____)\n",
    "iplot(____)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting findings! Now answer the following questions:\n",
    "\n",
    "- Which cities are the closest to these top 10 locations?\n",
    "- Which states in the United States are these top coordinates located in?\n",
    "- What landmarks - airports, research centres, etc. - do these coordinates correlate with?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your answers here\n",
    "cities = [\"___\", \"___\", \"___\", \"___\", \"___\", \"___\", \"___\", \"___\", \"___\", \"___\"]\n",
    "us_states = [\"___\", \"___\", \"___\", \"___\", \"___\", \"___\"]\n",
    "landmarks = [\"___\", \"___\", \"___\", \"___\", \"___\", \"___\", \"___\", \"___\", \"___\", \"___\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONGRATULATIONS!!!\n",
    "Well done on completing this difficult part of the assignment. All is now left for you to do is to visualise the model outputs you have saved in the `ten_locations_kmeans.csv` file in your S3 bucket on a map. Simply create an **AWS Quicksight** account and use the `my-manifest.json` file under the `quicksight` folder of [BML github repo](https://github.com/beginners-machine-learning-london/intro_to_unsupervised_ml_with_AWS_Sagemaker/tree/master/exercises/quicksight) to configure AWS Quicksight.\n",
    "\n",
    "Again, Well done on completing the above assignments! This was a hard exercise. You have learned how to use AWS Sagemaker to train an unsupervised machine learning model in the cloud. We hope that you enjoyed this **Introduction to unsupervised machine learning with AWS** Workshop. To learn more about AWS Sagemaker and machine learning in the cloud check out a few resources we have provided in our repo's [README.md](https://github.com/beginners-machine-learning-london/intro_to_unsupervised_ml_with_AWS_Sagemaker).\n",
    "\n",
    "Also make sure to join our meetup group to be informed of future workshops! [London Beginners Machine Learning Meetup](https://www.meetup.com/beginners-machine-learning-london/).\n",
    "\n",
    "And join our [slack channel](https://join.slack.com/t/beginnersmach-wlf5812/shared_invite/enQtNzAzODA4OTY3MTcyLWU2ZDMzNGU2YTQ4ZDk5ZjY3OTk1YWU2OGU5NWRmMjM1NzkwM2MwYjk5MDNhZWE1YWVmNzY1MjgzZDk4OGE1OGE) to ask questions, discuss ML with other BML community members and suggest the topics of future workshops."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
